{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  \n",
    "1. How does unsqueeze help us to solve certain broadcasting problems?**\n",
    "\n",
    "unsqueeze adds a new dimension to a tensor, which allows broadcasting to\n",
    "be applied more easily. For example, unsqueeze(a, 0) creates a new\n",
    "dimension at the beginning of the tensor, allowing it to be broadcast\n",
    "with another tensor with a different number of dimensions.\n",
    "\n",
    "**2. How can we use indexing to do the same operation as unsqueeze?**\n",
    "\n",
    "We can use slicing to do the same operation as unsqueeze, by creating a\n",
    "new dimension at the beginning or end of the tensor.\n",
    "\n",
    "**3. How do we show the actual contents of the memory used for a\n",
    "tensor?**\n",
    "\n",
    "To show the actual contents of the memory used for a tensor, we can use\n",
    "the storage attribute of the tensor, which returns a flattened version\n",
    "of the tensor.\n",
    "\n",
    "**4. When adding a vector of size 3 to a matrix of size 3×3, are the\n",
    "elements of the vector added to each row or each column of the matrix?\n",
    "(Be sure to check your answer by running this code in a notebook.)**\n",
    "\n",
    "When adding a vector of size 3 to a matrix of size 3×3, the elements of\n",
    "the vector are added to each column of the matrix. This is because\n",
    "broadcasting is applied along the rows of the matrix.\n",
    "\n",
    "**5. Do broadcasting and expand_as result in increased memory use? Why\n",
    "or why not?**\n",
    "\n",
    "Broadcasting and expand_as do not result in increased memory use, as\n",
    "they do not create new tensors. They simply change the way the existing\n",
    "tensors are broadcast and combined.\n",
    "\n",
    "**7. What does a repeated index letter represent on the lefthand side\n",
    "of einsum?**\n",
    "\n",
    "A repeated index letter on the lefthand side of einsum represents a\n",
    "summation over that index. For example, if the lefthand side is\n",
    "'ij,jk->ik', then the repeated index letter j indicates that the\n",
    "summation is over the j index.\n",
    "\n",
    "**8. What are the three rules of Einstein summation notation? Why?**\n",
    "\n",
    "The three rules of Einstein summation notation are:\n",
    "\n",
    "Repeated indices are summed over. Indices that appear only once are left\n",
    "unchanged. The order of the indices can be changed as long as the order\n",
    "is the same on both the lefthand and righthand sides.\n",
    "\n",
    "**9. What are the forward pass and backward pass of a neural network?**\n",
    "\n",
    "The forward pass is the process of feeding input data through a neural\n",
    "network to generate an output. During the forward pass, each neuron in\n",
    "the network receives input and applies an activation function to produce\n",
    "an output, which is then passed to the next layer until the final output\n",
    "is generated. The backward pass (also called backpropagation) is the\n",
    "process of computing the gradients of the loss function with respect to\n",
    "the weights of the neural network, which is used to update the weights\n",
    "through optimization algorithms such as stochastic gradient descent.\n",
    "\n",
    "**10. Why do we need to store some of the activations calculated for\n",
    "intermediate layers in the forward pass?**\n",
    "\n",
    "Storing activations from intermediate layers during the forward pass is\n",
    "necessary for computing the gradients during the backward pass. The\n",
    "gradients are computed by backpropagating errors from the output layer\n",
    "to the input layer through the chain rule, which involves calculating\n",
    "the derivative of the activation function at each layer. The\n",
    "intermediate activations are needed to compute these derivatives\n",
    "accurately.\n",
    "\n",
    "**11. What is the downside of having activations with a standard\n",
    "deviation too far away from 1?**\n",
    "\n",
    "If the standard deviation of the activations in a neural network is too\n",
    "far away from 1, it can lead to vanishing or exploding gradients.\n",
    "Vanishing gradients occur when the gradient signal becomes too small,\n",
    "making it difficult for the optimizer to update the weights. Exploding\n",
    "gradients occur when the gradient signal becomes too large, causing the\n",
    "optimization algorithm to overshoot the optimal weights.\n",
    "\n",
    "**12. How can weight initialization help avoid this problem?**\n",
    "\n",
    "Proper weight initialization can help avoid the problem of vanishing or\n",
    "exploding gradients. One common technique is to use Xavier or He\n",
    "initialization, which initializes the weights with a Gaussian\n",
    "distribution that takes into account the size of the input and output\n",
    "layers of each neuron. This helps to balance the initial scale of the\n",
    "activations, preventing them from becoming too small or too large during\n",
    "training."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
