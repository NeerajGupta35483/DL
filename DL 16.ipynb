{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  \n",
    "1. Explain the Activation Functions in your own language**\n",
    "\n",
    "> a\\) sigmoid\n",
    ">\n",
    "> b\\) tanh\n",
    ">\n",
    "> c\\) ReLU\n",
    ">\n",
    "> d\\) ELU\n",
    ">\n",
    "> e\\) LeakyReLU\n",
    ">\n",
    "> f\\) swish\n",
    "\n",
    "a\\) The sigmoid activation function maps any input value to a value\n",
    "between 0 and 1, which makes it useful for binary classification\n",
    "problems. However, it can cause the vanishing gradient problem and is\n",
    "not recommended for deep neural networks.\n",
    "\n",
    "b\\) The tanh activation function maps any input value to a value between\n",
    "-1 and 1, which can help alleviate the vanishing gradient problem\n",
    "compared to sigmoid. However, it can still suffer from the same issue\n",
    "and is not commonly used in deep neural networks.\n",
    "\n",
    "c\\) The ReLU (Rectified Linear Unit) activation function returns 0 for\n",
    "any negative input and the input value for any positive input, which\n",
    "makes it computationally efficient and effective for deep neural\n",
    "networks. However, it can suffer from the \"dying ReLU\" problem, where\n",
    "some neurons may become inactive and stop learning.\n",
    "\n",
    "d\\) The ELU (Exponential Linear Unit) activation function is similar to\n",
    "ReLU for positive inputs, but smoothly transitions to a negative value\n",
    "for negative inputs. It can help alleviate the dying ReLU problem and\n",
    "has been shown to improve the accuracy of deep neural networks.\n",
    "\n",
    "e\\) The LeakyReLU activation function is similar to ReLU for positive\n",
    "inputs, but returns a small negative value for negative inputs. This can\n",
    "help prevent the dying ReLU problem and improve the accuracy of deep\n",
    "neural networks.\n",
    "\n",
    "f\\) The swish activation function is a recently proposed function that\n",
    "applies a sigmoid-like function to the input multiplied by the input\n",
    "value. It has shown promising results in some experiments and is being\n",
    "studied as a potential replacement for ReLU.\n",
    "\n",
    "**2. What happens when you increase or decrease the optimizer learning\n",
    "rate?**\n",
    "\n",
    "Increasing the optimizer learning rate can result in faster convergence\n",
    "during training, but can also cause the model to overshoot the optimal\n",
    "solution and diverge. Decreasing the learning rate can slow down\n",
    "convergence but can lead to better optimization and performance.\n",
    "\n",
    "**3. What happens when you increase the number of internal hidden\n",
    "neurons?**\n",
    "\n",
    "Increasing the number of internal hidden neurons can allow the model to\n",
    "capture more complex patterns in the data, but can also lead to\n",
    "overfitting and slower training times. Finding the optimal number of\n",
    "neurons typically requires experimentation and validation.\n",
    "\n",
    "**4. What happens when you increase the size of batch computation?**\n",
    "\n",
    "Increasing the size of batch computation can improve training efficiency\n",
    "and reduce noise in the parameter updates, but can also result in slower\n",
    "convergence and may require more memory to store intermediate\n",
    "computations.\n",
    "\n",
    "**5. Why we adopt regularization to avoid overfitting?**\n",
    "\n",
    "Regularization is used to prevent overfitting by adding constraints to\n",
    "the model parameters or by adding penalties to the loss function. This\n",
    "helps the model generalize better to new data and prevent memorization\n",
    "of the training set.\n",
    "\n",
    "**6. What are loss and cost functions in deep learning?**\n",
    "\n",
    "In deep learning, the loss function measures how well the model is\n",
    "performing on the training data by comparing the predicted output to the\n",
    "actual output. The cost function is the average loss over the entire\n",
    "training set, and is used to optimize the model parameters during\n",
    "training.\n",
    "\n",
    "**7. What do ou mean by underfitting in neural networks?**\n",
    "\n",
    "Underfitting in neural networks occurs when the model is too simple to\n",
    "capture the underlying patterns in the data, resulting in poor\n",
    "performance on both the training and validation sets. This can happen\n",
    "when the model has too few hidden neurons or is not trained for long\n",
    "enough.\n",
    "\n",
    "**8. Why we use Dropout in Neural Networks?**\n",
    "\n",
    "Dropout is used in neural networks as a regularization technique to\n",
    "prevent overfitting. It randomly drops out a fraction of the neurons\n",
    "during each training iteration, forcing the network to learn more robust\n",
    "features and reducing the risk of memorization."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
