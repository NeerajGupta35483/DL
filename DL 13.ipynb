{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  \n",
    "1. Why is it generally preferable to use a Logistic Regression\n",
    "classifier rather than a classical Perceptron (i.e., a single layer of\n",
    "linear threshold units trained using the Perceptron training algorithm)?\n",
    "How can you tweak a Perceptron to make it equivalent to a Logistic\n",
    "Regression classifier?**\n",
    "\n",
    "Logistic Regression is generally preferable to a classical Perceptron\n",
    "because it can output class probabilities and handle non-linearly\n",
    "separable data through the use of a logistic activation function. To\n",
    "make a Perceptron equivalent to a Logistic Regression classifier, you\n",
    "can replace the step activation function with a logistic function and\n",
    "use gradient descent to minimize the cross-entropy loss.\n",
    "\n",
    "**2. Why was the logistic activation function a key ingredient in\n",
    "training the first MLPs?**\n",
    "\n",
    "The logistic activation function was a key ingredient in training the\n",
    "first MLPs because it allowed for non-linear transformations of the\n",
    "input data and smooth gradients that made backpropagation feasible. The\n",
    "logistic function is also differentiable, allowing for the use of\n",
    "gradient descent optimization.\n",
    "\n",
    "**3. Name three popular activation functions. Can you draw them?**\n",
    "\n",
    "Three popular activation functions are the sigmoid function, ReLU\n",
    "(rectified linear unit) function, and tanh (hyperbolic tangent)\n",
    "function. The sigmoid function is a smooth S-shaped curve that outputs\n",
    "values between 0 and 1. ReLU is a piecewise linear function that outputs\n",
    "0 for negative inputs and the input itself for positive inputs. tanh is\n",
    "similar to sigmoid, but outputs values between -1 and 1.\n",
    "\n",
    "**4. Suppose you have an MLP composed of one input layer with 10\n",
    "passthrough neurons, followed by one hidden layer with 50 artificial\n",
    "neurons, and finally one output layer with 3 artificial neurons. All\n",
    "artificial neurons use the ReLU activation function.**\n",
    "\n",
    "• What is the shape of the input matrix X?\n",
    "\n",
    "• What about the shape of the hidden layer’s weight vector Wh, and the\n",
    "shape of its bias vector bh?\n",
    "\n",
    "• What is the shape of the output layer’s weight vector Wo, and its bias\n",
    "vector bo?\n",
    "\n",
    "• What is the shape of the network’s output matrix Y?\n",
    "\n",
    "• Write the equation that computes the network’s output matrix Y as a\n",
    "function of X, Wh, bh, Wo and bo.\n",
    "\n",
    "1.  The input matrix X has a shape of (m, 10), where m is the number of\n",
    "    training examples and 10 is the number of input features.\n",
    "\n",
    "2.  The hidden layer's weight vector Wh has a shape of (10, 50), where\n",
    "    10 is the number of inputs and 50 is the number of neurons in the\n",
    "    hidden layer. The bias vector bh has a shape of (50,), where 50 is\n",
    "    the number of neurons in the hidden layer.\n",
    "\n",
    "3.  The output layer's weight vector Wo has a shape of (50, 3), where 50\n",
    "    is the number of neurons in the hidden layer and 3 is the number of\n",
    "    neurons in the output layer. The bias vector bo has a shape of (3,),\n",
    "    where 3 is the number of neurons in the output layer.\n",
    "\n",
    "4.  The network's output matrix Y has a shape of (m, 3), where m is the\n",
    "    number of training examples and 3 is the number of output neurons.\n",
    "\n",
    "5.  The equation that computes the network's output matrix Y as a\n",
    "    function of X, Wh, bh, Wo, and bo is:\n",
    "\n",
    "Y = ReLU(ReLU(X.dot(Wh) + bh).dot(Wo) + bo)\n",
    "\n",
    "Here, the dot product (dot) represents matrix multiplication, and ReLU\n",
    "is the rectified linear activation function applied element-wise to the\n",
    "input matrix.\n",
    "\n",
    "**5. How many neurons do you need in the output layer if you want to\n",
    "classify email into spam or ham? What activation function should you use\n",
    "in the output layer? If instead you want to tackle MNIST, how many\n",
    "neurons do you need in the output layer, using what activation\n",
    "function?**\n",
    "\n",
    "For binary classification of spam/ham emails, you only need one neuron\n",
    "in the output layer with a sigmoid activation function that outputs a\n",
    "probability between 0 and 1. For MNIST, you need 10 neurons in the\n",
    "output layer (one for each class) with a softmax activation function to\n",
    "output a probability distribution over the 10 classes.\n",
    "\n",
    "**6. What is backpropagation and how does it work? What is the\n",
    "difference between backpropagation and reverse-mode autodiff?**\n",
    "\n",
    "Backpropagation is an algorithm used to calculate the gradients of the\n",
    "loss function with respect to the weights in a neural network. It works\n",
    "by recursively applying the chain rule of calculus to compute the\n",
    "derivatives of the output with respect to each weight. Reverse-mode\n",
    "autodiff is a more general technique used to calculate gradients of\n",
    "arbitrary functions, but it is equivalent to backpropagation in neural\n",
    "networks.\n",
    "\n",
    "**7. Can you list all the hyperparameters you can tweak in an MLP? If\n",
    "the MLP overfits the training data, how could you tweak these\n",
    "hyperparameters to try to solve the problem?**\n",
    "\n",
    "Some of the hyperparameters that can be tuned in a multi-layer\n",
    "perceptron (MLP) include the number of hidden layers, the number of\n",
    "neurons in each layer, the activation function for each layer, the\n",
    "learning rate, the optimization algorithm, and the regularization\n",
    "strength. If the MLP overfits the training data, you can try reducing\n",
    "the number of neurons or layers, increasing regularization, or applying\n",
    "dropout regularization to prevent overfitting.\n",
    "\n",
    "**8. Train a deep MLP on the MNIST dataset and see if you can get over\n",
    "98% precision. Try adding all the bells and whistles (i.e., save\n",
    "checkpoints, restore the last checkpoint in case of an interruption, add\n",
    "summaries, plot learning curves using TensorBoard, and so on).**"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
