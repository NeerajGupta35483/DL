{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  \n",
    "1. What are the pros and cons of using a stateful RNN versus a stateless\n",
    "RNN?**\n",
    "\n",
    "A stateful RNN is good for processing long sequences and preserving\n",
    "context across batches, but it is more difficult to parallelize and\n",
    "prone to overfitting. A stateless RNN is simpler to implement,\n",
    "parallelize, and less prone to overfitting, but it cannot preserve\n",
    "context across batches.\n",
    "\n",
    "**2. Why do people use Encoderâ€“Decoder RNNs rather than plain\n",
    "sequence-to-sequence RNNs for automatic translation?**\n",
    "\n",
    "Encoder-Decoder RNNs are used for automatic translation because they can\n",
    "handle variable-length input and output sequences, and the encoder can\n",
    "capture the meaning of the input sequence and the decoder can generate a\n",
    "target sequence based on that meaning.\n",
    "\n",
    "**3. How can you deal with variable-length input sequences? What about\n",
    "variable-length output sequences?**\n",
    "\n",
    "To deal with variable-length input sequences, you can use padding,\n",
    "masking, or bucketing. To handle variable-length output sequences, you\n",
    "can use teacher forcing, scheduled sampling, or beam search.\n",
    "\n",
    "**4. What is beam search and why would you use it? What tool can you use\n",
    "to implement it?**\n",
    "\n",
    "Beam search is a decoding algorithm that finds the most likely output\n",
    "sequence given a trained model. It considers multiple hypotheses in\n",
    "parallel and keeps the top-K most likely hypotheses at each step. The\n",
    "Python library, NLTK, provides an implementation of beam search.\n",
    "\n",
    "**5. What is an attention mechanism? How does it help?**\n",
    "\n",
    "An attention mechanism is a way for a model to selectively focus on\n",
    "different parts of an input sequence when generating an output. It helps\n",
    "the model to better understand the context and relationships between\n",
    "different parts of the input sequence, resulting in better performance.\n",
    "\n",
    "**6. What is the most important layer in the Transformer architecture?\n",
    "What is its purpose?**\n",
    "\n",
    "The most important layer in the Transformer architecture is the\n",
    "self-attention layer. It allows the model to attend to different parts\n",
    "of the input sequence and capture long-term dependencies without the\n",
    "need for recurrence.\n",
    "\n",
    "**7. When would you need to use sampled softmax?**\n",
    "\n",
    "Sampled softmax is used when there is a large number of output classes,\n",
    "making it impractical to compute the softmax function over all classes\n",
    "at each timestep. It randomly samples a subset of the output classes and\n",
    "computes the softmax over that subset, resulting in faster and more\n",
    "efficient training."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
