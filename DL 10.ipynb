{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  \n",
    "1. What does a SavedModel contain? How do you inspect its content?**\n",
    "\n",
    "A SavedModel contains a trained TensorFlow model and its associated\n",
    "variables, assets, and metadata. You can inspect its content using the\n",
    "saved_model_cli command-line interface.\n",
    "\n",
    "**2. When should you use TF Serving? What are its main features? What\n",
    "are some tools you can use to deploy it?**\n",
    "\n",
    "TF Serving is used to serve TensorFlow models at scale in production\n",
    "environments. Its main features include high performance, versioning,\n",
    "model rollback, and monitoring. Tools to deploy it include Docker and\n",
    "Kubernetes.\n",
    "\n",
    "**3. How do you deploy a model across multiple TF Serving instances?**\n",
    "\n",
    "You can deploy a model across multiple TF Serving instances using\n",
    "Kubernetes and Istio for service mesh.\n",
    "\n",
    "**4. When should you use the gRPC API rather than the REST API to query\n",
    "a model served by TF Serving?**\n",
    "\n",
    "You should use the gRPC API rather than the REST API to query a model\n",
    "served by TF Serving when you need low latency and high throughput.\n",
    "\n",
    "**5. What are the different ways TFLite reduces a model’s size to make\n",
    "it run on a mobile or embedded device?**\n",
    "\n",
    "TFLite reduces a model's size for mobile or embedded devices by\n",
    "quantization, pruning, and compression.\n",
    "\n",
    "**6. What is quantization-aware training, and why would you need it?**\n",
    "\n",
    "Quantization-aware training is a technique to train a model that is\n",
    "robust to low-precision inference by simulating the quantization effects\n",
    "during training.\n",
    "\n",
    "**7. What are model parallelism and data parallelism? Why is the latter\n",
    "generally recommended?**\n",
    "\n",
    "Model parallelism and data parallelism are ways to distribute the\n",
    "computation of a model across multiple devices or servers. Data\n",
    "parallelism is generally recommended because it is simpler and can\n",
    "handle larger batch sizes.\n",
    "\n",
    "**8. When training a model across multiple servers, what distribution\n",
    "strategies can you use? How do you choose which one to use?**\n",
    "\n",
    "Distribution strategies to train a model across multiple servers include\n",
    "synchronous, asynchronous, and parameter server. The choice depends on\n",
    "the size of the model, the number of servers, the network bandwidth, and\n",
    "the communication latency."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
